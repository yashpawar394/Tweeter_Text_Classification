# -*- coding: utf-8 -*-
"""Copy of twitter-sentiment-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17FhbwrkYYfcA0JNj2mUWy4mw-MTACtoh
"""

!pip install emoji

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM

from keras import utils
from keras.optimizers import Adam

import tensorflow as tf

import emoji
import nltk 
import re 
import string 
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud,STOPWORDS

from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/MyDrive/training.1600000.processed.noemoticon.csv.zip'

data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None)

"""# **Exploratory data analysis**"""

data.head(10)

"""By looking at the top part of the dataset we can learn a lot from it. We can indicate which column refers to. Therefore, we can describe them briefly:


*   0 - target of sentiment
*   1 - id of user
*   2 - date of tweet
*   3 -  'NO_QUERY'
*   4 - username
*   5 - tweet

### **Renaming column names**


Because of the numerical column names, it will be more convenient to work with a dataset with predefined column names.
"""

data = data.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'})

data.head(10)

"""### **Targets**



Firstly let's see what the classes of the individual tweets are about.
"""

pd.set_option('display.max_colwidth', -1)
data[data['target']==0]['content'].head()

"""By reading the content of the tweets, we can conclude that they have a rather negative message, so class 0 refers to negative sentiments tweets."""

data[data['target']==4]['content'].head

"""By reading the content of the tweets, we can conclude that they have a rather positive message, so class 4 refers to positive sentiments tweets."""

data['target'] = data['target'].replace([0, 4],['Negative','Positive'])

data['target'].value_counts()

sns.countplot(x='target',hue='target',data=data)

"""As we can see dataset is perfectly balanced with the same numbers of occurrences for both classes. It is also worth mentioning that the data is not skewed which will certainly make modeling easier."""

data['target'].values

fig, ax = plt.subplots()
ax.pie(data['target'].value_counts(), labels =['Negative','Positive'] , autopct='%1.1f%%')
plt.show()

"""### **Length of tweet content**

Based on this analysis, we can find out the length of tweets for two particular classes of tweets.
"""

data['length'] = data.content.str.split().apply(len)

"""data[data['username']=='what_bugs_u'].head()

# Distribution of text length for positive and Negative sentiment tweets
"""

fig = plt.figure(figsize=(14,7))
data['length'] = data.content.str.split().apply(len)
ax1 = fig.add_subplot(122)
sns.histplot(data[data['target']=='Positive']['length'], ax=ax1,color='green')
describe = data.length[data.target=='Positive'].describe().to_frame().round(2)

ax2 = fig.add_subplot(121)
ax2.axis('off')
font_size = 14
bbox = [0, 0, 1, 1]
table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)
table.set_fontsize(font_size)
fig.suptitle('Distribution of text length for positive sentiment tweets.', fontsize=16)

plt.show()

fig = plt.figure(figsize=(14,7))
data['length'] = data.content.str.split().apply(len)
ax1 = fig.add_subplot(122)
sns.histplot(data[data['target']=='Negative']['length'], ax=ax1,color='red')
describe = data.length[data.target=='Negative'].describe().to_frame().round(2)

ax2 = fig.add_subplot(121)
ax2.axis('off')
font_size = 14
bbox = [0, 0, 1, 1]
table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)
table.set_fontsize(font_size)
fig.suptitle('Distribution of text length for Negative sentiment tweets.', fontsize=16)

plt.show()

"""### **Wordclouds**

By creating word clouds for two classes, we can visualize what words were repeated most often for positive and negative classes. We don't want to show stopwords so i took base of stopwords from nltk library and i passed it to WordCloud function
"""

plt.figure(figsize=(14,7))
word_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color="white").generate(" ".join(data[data.target=='Positive'].content))
plt.imshow(word_cloud,interpolation='bilinear')
plt.axis('off')
plt.title('Most common words in positive sentiment tweets.',fontsize=20)
plt.show()

"""Based on the word cloud, it can be deduced that the most repeated words in tweets with positive sentiment are words such as: love, quot, lol, haha, thank, today.

Based on the word cloud, it can be deduced that the most repeated words in tweets with negative sentiment are words such as: quot, lol, today which are the same as for positive sentiment class. However, there are also word occurrences from which negative sentiment of a tweet can be inferred such as: miss, sorry, hate etc.

# **Data preparing**
### **Dropping unnecessary columns**


There are a lot of unnecessary columns in the following dataset. The task is to classify the semantics of a tweet, so all columns except the target and content columns are unnecessary.
"""

data=data[['target','content']]

data.head()

data.target = data.target.replace({'Positive': 1, 'Negative': 0})

"""Replacing Positive and Negative labels with 1 and 0 respectively.

### **Content cleaning**

Stemming - it does refers to the process which goal is to reduce words into thier base form. In case of our problem for classification it is very important ooperation as we need to focus on the meaning of particular word. For instance words: *Running, Runned, Runner* all can reduce to the stem *Run*. Below we have used the base of english stopwords and stemming algorithm from nltk library.
"""

wnl = WordNetLemmatizer()
nltk.download('wordnet')
nltk.download('omw-1.4')

def data_preprocesing(tweet):
  tokens = []

  for token in tweet.split():
    tokens.append(wnl.lemmatize(token,pos="v"))
  tweet =  " ".join(tokens)
  
    
  return "".join(tweet)
  


def replace_contractions(t):
  '''
  This function replaces english lanuage contractions like "shouldn't" with "should not"
  '''
  cont = {"aren't" : 'are not', "can't" : 'cannot', "couln't": 'could not', "didn't": 'did not', "doesn't" : 'does not',
  "hadn't": 'had not', "haven't": 'have not', "he's" : 'he is', "she's" : 'she is', "he'll" : "he will", 
  "she'll" : 'she will',"he'd": "he would", "she'd":"she would", "here's" : "here is", 
   "i'm" : 'i am', "i've"	: "i have", "i'll" : "i will", "i'd" : "i would", "isn't": "is not", 
   "it's" : "it is", "it'll": "it will", "mustn't" : "must not", "shouldn't" : "should not", "that's" : "that is", 
   "there's" : "there is", "they're" : "they are", "they've" : "they have", "they'll" : "they will",
   "they'd" : "they would", "wasn't" : "was not", "we're": "we are", "we've":"we have", "we'll": "we will", 
   "we'd" : "we would", "weren't" : "were not", "what's" : "what is", "where's" : "where is", "who's": "who is",
   "who'll" :"who will", "won't":"will not", "wouldn't" : "would not", "you're": "you are", "you've":"you have",
   "you'll" : "you will", "you'd" : "you would", "mayn't" : "may not"}
  words = t.split()
  reformed = []
  for w in words:
    if w in cont:
      reformed.append(cont[w])
    else:
      reformed.append(w)
  t = " ".join(reformed)
  return t  

def remove_single_letter_words(t):
  '''
  This function removes words that are single characters
  '''
  words = t.split()
  reformed = []
  for w in words:
    if len(w) > 1:
      reformed.append(w)
  t = " ".join(reformed)
  return t  

def de_emoji(tweet):
  tweet = emoji.demojize(tweet)
  tweet = tweet.replace(":"," ")
  return ' '.join(tweet.split())


def dataclean(t):
  '''
  This function cleans the tweets.
  '''
  t = t.lower() # convert to lowercase
  t = replace_contractions(t) # replace short forms used in english  with their actual words
  t = de_emoji(t) # replace unicode emojis with their feeling associated
  t = re.sub('\\\\u[0-9A-Fa-f]{4}','', t) # remove NON- ASCII characters
  t = re.sub("[0-9]", "", t) # remove numbers # re.sub("\d+", "", t)
  t = re.sub('#', '', t) # remove '#'
  t = re.sub('@[A-Za-z0â€“9]+', '', t) # remove '@'
  t = re.sub('@[^\s]+', '', t) # remove usernames
  t = re.sub('RT[\s]+', '', t) # remove retweet 'RT'
  t = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', '', t) # remove links (URLs/ links)
  t = re.sub('[!"$%&\'()*+,-./:@;<=>?[\\]^_`{|}~]', '', t) # remove punctuations
  t = t.replace('\\\\', '')
  t = t.replace('\\', '')
  t = remove_single_letter_words(t) # removes single letter words
  t = data_preprocesing(t)
  return t

data['content'] = data['content'].apply(dataclean)

data.head()

data.head()

"""Head of data after stemming and removing https.

### **Train test split**
Due to the rather large size of the dataset 160000 tweets will be enough for testing.
"""

x = data['content']

y = data['target']

train, test = train_test_split(data, test_size=0.1, random_state=44)

print('Train dataset shape: {}'.format(train.shape))
print('Test dataset shape: {}'.format(test.shape))

"""### **Tokenization**

It is a particular kind of document segmentation. It does breaks up text into smaller chunks or segments called tokens. A tokenizer breaks unstructured data, natural language text, into chunks of information that can be counted as discrete elements. After this operation these counts of token occurences in particular document can be used as a vector representing given document.
"""

train

train.shape

train.to_csv('/content/drive/MyDrive/ml_models/train_data.csv', index=False)

my_train = pd.read_csv("/content/drive/MyDrive/ml_models/train_data.csv",squeeze=True)

train.info()

my_train.info()

my_train

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train.content)  
vocab_size = len(tokenizer.word_index) + 1 
max_length = 50

"""Based on the analysis of the tweet length it was concluded that the maximum length for tokenization equal to 50 will be sufficient"""

sequences_train = tokenizer.texts_to_sequences(train.content) 
sequences_test = tokenizer.texts_to_sequences(test.content) 

X_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')
X_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')

y_train = train.target.values
y_test = test.target.values

"""### **Word embeddings using GloVe**

Word embeddings provide a dense representation of words and their relative meanings. Embedding Matrix is a maxtrix of all words and their corresponding embeddings. Embedding matrix is used in embedding layer in model to embedded a token into it's vector representation, that contains information regarding that token or word.

Embedding vocabulary is taken from the tokenizer and the corresponding vectors from embedding model, which in this case is GloVe model. GloVe stand for Global Vectors for Word Representation 
and it is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.

Below was used pretrained GloVe embeddings from world known Stanford vector files. The smallest available file contains embeddings created for tiny 6 billions of tokens.
"""

# !wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip "/content/drive/MyDrive/glove/glove.6B.zip"

embeddings_dictionary = dict()
embedding_dim = 100
glove_file = open('/content/glove.6B.100d.txt')

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
    
glove_file.close()

embeddings_matrix = np.zeros((vocab_size, embedding_dim))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embeddings_matrix[index] = embedding_vector

embeddings_dictionary

embeddings_matrix

"""# **Model test harness** 

The proposed model architecture will be tested on the following parameters:

*   **loss** = "binnary_crossentropy" (due to binary classification problem)
*   **optimizer** = Adam(learning_rate=0.001) (may be changed after seeing the learning graph)
*   **metrics** = "accuracy" (due to binary classification problem)
*   **number of epochs** = 10 (due to the large training data set)
*   **batch size** = 1000 (in order to accelerate learning time)
"""

embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)

num_epochs = 5
batch_size = 1000

"""# **Model - Embedding + Stacked LSTM**


Model consisted of layers build with lstm cells. With such a large amount of data, the model is computationally complex making the training process take a while. Furthermore, model regularization layers will reduce the possible overfitting which was present in the simpler models tested.
"""

model = Sequential([
        embedding_layer,
        tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.Bidirectional(LSTM(128)),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid'),
    ])

model.summary()

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size = batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)

type(X_test)

y_pred = model.predict(X_test)
y_pred = np.where(y_pred>0.5, 1, 0)

print(classification_report(y_test, y_pred))

from mlxtend.plotting import plot_confusion_matrix

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)

fig, ax = plot_confusion_matrix(conf_mat = conf_matrix, figsize=(7.5, 12), cmap=plt.cm.Blues)
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actual', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

def predict_sentiment(tweet):
  tweet = dataclean(tweet)
  # tweet = tweet.split()
  # print(tweet)
  tweet = np.array([tweet])
  tweet_x = tokenizer.fit_on_texts(tweet) 
  print(tweet)
  tweet_x = tokenizer.texts_to_sequences(tweet)
  print(tweet_x)
  # tweet = tokenizer.text_to_sequences(tweet)
  tweet = pad_sequences(tweet_x,maxlen=max_length,padding='post')
  y_pred = model.predict(tweet)
  y_pred = np.where(y_pred>0.5, 1, 0)
  return y_pred

tweet = "Quite interesting!"
sentiment = predict_sentiment(tweet)
if sentiment[0][0] == 1:
  print("Positive ðŸ˜ƒ")
elif sentiment[0][0] == 0:
  print("Negative ðŸ˜¥")

tf.keras.models.save_model(model,"/content/drive/MyDrive/ml_models/new_tsa_model.h5")