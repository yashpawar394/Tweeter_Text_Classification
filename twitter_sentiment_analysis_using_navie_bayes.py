# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis_Using_Navie_Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CPKLtVgRl8wkrDdn0mkNPu1KoPV5MVoG
"""

print("Importing the modules...")
import pandas as pd
import re,string
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
import numpy as np
import nltk 
from nltk.stem import WordNetLemmatizer
import pickle
import emoji

print("Importing completed...")
print("Reading the data...")
column_names = ['target','ids','date','flag','user','text']
df = pd.read_csv("E:\\Tweeter_troll_detection\\trial\\training.1600000.processed.noemoticon.csv",delimiter=",",names=column_names,encoding='latin-1')

"""Column Notation 

***0 ==> Negative***

***4 ==> Positive*** 
"""

df['target'] = df['target'].replace([0, 4],['Negative','Positive'])


df['target'] = df['target'].replace(['Negative','Positive'],[0, 1])

df_final = df[['target','text']]
print("Reading completed...")

wnl = WordNetLemmatizer()
nltk.download('wordnet')
nltk.download('omw-1.4')



#------------------------------------------------------------------------------------------------------------------

def data_preprocesing(tweet):
  tokens = []

  for token in tweet.split():
    tokens.append(wnl.lemmatize(token,pos="v"))
  tweet =  " ".join(tokens)
  
    
  return "".join(tweet)
  


def replace_contractions(t):
  '''
  This function replaces english lanuage contractions like "shouldn't" with "should not"
  '''
  cont = {"aren't" : 'are not', "can't" : 'cannot', "couln't": 'could not', "didn't": 'did not', "doesn't" : 'does not',
  "hadn't": 'had not', "haven't": 'have not', "he's" : 'he is', "she's" : 'she is', "he'll" : "he will", 
  "she'll" : 'she will',"he'd": "he would", "she'd":"she would", "here's" : "here is", 
   "i'm" : 'i am', "i've"	: "i have", "i'll" : "i will", "i'd" : "i would", "isn't": "is not", 
   "it's" : "it is", "it'll": "it will", "mustn't" : "must not", "shouldn't" : "should not", "that's" : "that is", 
   "there's" : "there is", "they're" : "they are", "they've" : "they have", "they'll" : "they will",
   "they'd" : "they would", "wasn't" : "was not", "we're": "we are", "we've":"we have", "we'll": "we will", 
   "we'd" : "we would", "weren't" : "were not", "what's" : "what is", "where's" : "where is", "who's": "who is",
   "who'll" :"who will", "won't":"will not", "wouldn't" : "would not", "you're": "you are", "you've":"you have",
   "you'll" : "you will", "you'd" : "you would", "mayn't" : "may not"}
  words = t.split()
  reformed = []
  for w in words:
    if w in cont:
      reformed.append(cont[w])
    else:
      reformed.append(w)
  t = " ".join(reformed)
  return t  

def remove_single_letter_words(t):
  '''
  This function removes words that are single characters
  '''
  words = t.split()
  reformed = []
  for w in words:
    if len(w) > 1:
      reformed.append(w)
  t = " ".join(reformed)
  return t  

def de_emoji(tweet):
  tweet = emoji.demojize(tweet)
  tweet = tweet.replace(":"," ")
  return ' '.join(tweet.split())


def dataclean(t):
  '''
  This function cleans the tweets.
  '''
  t = t.lower() # convert to lowercase
  t = replace_contractions(t) # replace short forms used in english  with their actual words
  t = de_emoji(t) # replace unicode emojis with their feeling associated
  t = re.sub('\\\\u[0-9A-Fa-f]{4}','', t) # remove NON- ASCII characters
  t = re.sub("[0-9]", "", t) # remove numbers # re.sub("\d+", "", t)
  t = re.sub('#', '', t) # remove '#'
  t = re.sub('@[A-Za-z0â€“9]+', '', t) # remove '@'
  t = re.sub('@[^\s]+', '', t) # remove usernames
  t = re.sub('RT[\s]+', '', t) # remove retweet 'RT'
  t = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', '', t) # remove links (URLs/ links)
  t = re.sub('[!"$%&\'()*+,-./:@;<=>?[\\]^_`{|}~]', '', t) # remove punctuations
  t = t.replace('\\\\', '')
  t = t.replace('\\', '')
  t = remove_single_letter_words(t) # removes single letter words
  t = data_preprocesing(t)
  global i
  i+=1
  print(i)
  
  return t

#----------------------------------------------------------------------------------------------------------------




print("Applying preprocessing...")
i = 0
df_final['text'] = df_final['text'].apply(dataclean)
print("preprocessing completed...")



print("Spliting the data....")
from sklearn.model_selection import train_test_split
x = df_final['text']
y = df_final['target']
x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.1, random_state=44)
print("Spliting completed....")
type(x_train[0])

x_train.shape

x_train[0],x_train[1]

from sklearn.feature_extraction.text import CountVectorizer
vectoriser = CountVectorizer()
print("Applying the vectoriser...")
vectoriser.fit(x_train)
pickle.dump(vectoriser,open("E:\\Tweeter_troll_detection\\trial\\new_nlpcountvectorizer.pkl","wb"))
x_train = vectoriser.transform(x_train)
x_test  = vectoriser.transform(x_test)
print("Applied vectoriser...")
#print('No. of feature_words: ', len(vectoriser.get_feature_names()))



x_test.shape

print("Fitting the model...")
model2 = MultinomialNB()
model2.fit(x_train,y_train)
print("model fitted...")
print("Predicitng the output...")
y_pred = model2.predict(x_test)


from sklearn import metrics
print(metrics.classification_report(y_test,y_pred))

print(metrics.accuracy_score(y_test,y_pred))

conf_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)

def predict_sentiment(tweet):
  tweet = data_preprocesing(tweet)
  tweet = dataclean(tweet)
  tweet = vectoriser.transform([tweet])
  tweet.shape
  y_pred = model2.predict(tweet)
  return y_pred[0]

tweet = "this looks delicious..."
sentiment = predict_sentiment(tweet)
if sentiment == 1:
  print("Positive")
elif sentiment== 0:
  print("Negative")

pickle.dump(model2,open("E:\\Tweeter_troll_detection\\trial\\new_nbcountmodel.pkl","wb"))
